{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "xHa4EE03wJF4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import json\n",
        "model_name = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ruoyuxie/adversarial_mwps_generation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0B3dWnKhvHl",
        "outputId": "8310a5b8-cd76-47df-eb20-43bcece2e583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'adversarial_mwps_generation'...\n",
            "remote: Enumerating objects: 876, done.\u001b[K\n",
            "remote: Counting objects: 100% (876/876), done.\u001b[K\n",
            "remote: Compressing objects: 100% (551/551), done.\u001b[K\n",
            "remote: Total 876 (delta 15), reused 862 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (876/876), 1.76 MiB | 3.75 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_data_path = '/content/adversarial_mwps_generation/data/experimental_data/generated_adversarial_examples/gsm8k.json'\n",
        "import json\n",
        "with open(generated_data_path, 'r') as f:\n",
        "  data = json.loads(f.read())\n",
        "\n",
        "sample_input = data['2']['M2'][0][0]\n",
        "sample_output = data['2']['M2'][0][1]"
      ],
      "metadata": {
        "id": "_dgGnX2vhxXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = data['2']['M3'][0][0]\n",
        "sample_output = data['2']['M3'][0][1]"
      ],
      "metadata": {
        "id": "Cqqe_jtxh7Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UhUCcALbh9go",
        "outputId": "8a161ab3-18f2-4b98-feda-ca853f3eed86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chenny is 15 years old. Alyana is 6 years younger than Chenny. How old is Anne if she is 8 years older than Alyana?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4oAlOv0Hh0Bm",
        "outputId": "57032e93-1a37-4d14-e682-a02e29f29fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ssncrVkh1hs",
        "outputId": "566c348a-8ddf-4544-c13a-390ff1076af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_prompt = \"Chenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?\"\n",
        "purtubed_prompt = \"Chenny is 78 years old. Alyana is 21 years younger than Chenny. How old is Anne if she is 31 years older than Alyana?\"\n",
        "\n",
        "correct_ans = '65'\n",
        "purtubed_ans = '88'\n",
        "\n",
        "correct_ans_tokens = tokenizer.encode(str(correct_ans))\n",
        "purtubed_ans_tokens = tokenizer.encode(str(purtubed_ans))\n"
      ],
      "metadata": {
        "id": "p_HCyVx8djMo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_probs = []"
      ],
      "metadata": {
        "id": "_LZFrwoLnAGO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indirect_effect = {}\n",
        "boxed_token = torch.tensor((tokenizer.encode('boxed{')))\n"
      ],
      "metadata": {
        "id": "gFNPcyLwwSEF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temperature = 0.7 and top_p = 0.8 according to Qwen docs\n",
        "# https://github.com/QwenLM/Qwen2.5-Math\n",
        "torch.manual_seed(42)\n",
        "\n",
        "def inference_with_hidden_state_new(prompt, temperature=0.7, top_p=0.8, save_activations=False, patch_activations = False, patch_layer=0, activations={}):\n",
        "    \"\"\"Returns a dictionary with keys: 'sequences', 'logits', 'attentions', 'hidden_states'.\n",
        "\n",
        "    - sequences: The output token IDs, shape: (batch_size, output_len).\n",
        "    - logits: Logits for the last generated token, shape: (1, vocab_size).\n",
        "    - attentions: Attention outputs for the last token, shape: (num_layers, num_heads, seq_len, seq_len).\n",
        "    - hidden_states: Hidden states for the last token, shape: (num_layers, hidden_size).\n",
        "    \"\"\"\n",
        "    # Prepare the input\n",
        "    messages = [\n",
        "        # {\"role\": \"system\", \"content\": \"Please put your final answer within \\\\boxed{}.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "    input_ids = model_inputs.input_ids\n",
        "\n",
        "    global sample_probs\n",
        "    # Initialize variables to store outputs\n",
        "    sequences = input_ids\n",
        "    # last_logits = None\n",
        "    # last_attentions = None\n",
        "    # last_hidden_states = None\n",
        "\n",
        "    # Disable cache to get full attention outputs\n",
        "    model.config.use_cache = True\n",
        "    track = False\n",
        "    # Generate tokens one by one\n",
        "    stored_activations = {}\n",
        "    with torch.no_grad():\n",
        "        for _ in range(512):  # max_new_tokens\n",
        "            outputs = model(\n",
        "                input_ids=sequences,\n",
        "                # output_attentions=True,\n",
        "                # output_hidden_states=True,\n",
        "                # return_dict=True\n",
        "            )\n",
        "\n",
        "            # Get the logits for the next token\n",
        "            next_token_logits = outputs.logits[:, -1, :]  # Shape: (1, vocab_size)\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "\n",
        "            # Sample the next token\n",
        "            probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append the new token to the sequence\n",
        "            # prev_sequences = sequences.clone()\n",
        "            sequences = torch.cat([sequences, next_token_id], dim=-1)\n",
        "\n",
        "            if track and cnt < len(correct_ans_tokens):\n",
        "              pr = f'pr*_{patch_layer}' if patch_activations else 'pr'\n",
        "              indirect_effect[f'{pr}_{cnt}'] = probs[0, correct_ans_tokens[cnt]]\n",
        "              indirect_effect[f'{pr}!_{cnt}'] = probs[0, purtubed_ans_tokens[cnt]]\n",
        "              cnt +=1\n",
        "\n",
        "            if torch.all(sequences[0,-2:] == boxed_token.to('cuda')):\n",
        "              track = True\n",
        "              cnt = 0\n",
        "              sample_probs = probs\n",
        "\n",
        "            if track and torch.all(sequences[0,-1:] == torch.tensor(tokenizer.encode('}')).to('cuda')):\n",
        "              track = False\n",
        "              print('ggg')\n",
        "\n",
        "\n",
        "## ----------------------- Save activations ---------------------------\n",
        "            if save_activations and track and cnt == 0:\n",
        "              print('saving activations ..')\n",
        "              def hook_fn(name):\n",
        "                def hook(module, input, output):\n",
        "                    # Store the activation for this module\n",
        "                    stored_activations[name] = output.detach().clone()\n",
        "                return hook\n",
        "\n",
        "              # Register forward hook\n",
        "              hooks = []\n",
        "\n",
        "              try:\n",
        "\n",
        "                for layer in range(model.config.num_hidden_layers):\n",
        "                  print(layer)\n",
        "\n",
        "                  module = model.model.layers[layer].mlp\n",
        "                  handle = module.register_forward_hook(hook_fn(f'layer_{layer}'))\n",
        "                  hooks.append(handle)\n",
        "\n",
        "                # Run the forward\n",
        "                output = model(input_ids=sequences)\n",
        "              finally:\n",
        "                for h in hooks:\n",
        "                  h.remove()\n",
        "\n",
        "              return stored_activations\n",
        "\n",
        "## ----------------------- Patch activations -------------------------\n",
        "            if patch_activations and track and cnt == 0:\n",
        "              print('applying patching')\n",
        "              def hook_fn(name):\n",
        "                def hook(module, input, output):\n",
        "                    # Store the activation for this module\n",
        "                    # stored_activations[name] = output.detach().clone()\n",
        "                    output[:,-1,:] = activations[f'layer_{patch_layer}'][:,-1,:]\n",
        "                return hook\n",
        "\n",
        "              # Register forward hook\n",
        "              hooks = []\n",
        "\n",
        "\n",
        "\n",
        "              module = model.model.layers[patch_layer].mlp\n",
        "              handle = module.register_forward_hook(hook_fn(f'layer_{patch_layer}'))\n",
        "              hooks.append(handle)\n",
        "\n",
        "                # # Run the forward\n",
        "                # output = model(input_ids=sequences)\n",
        "              # finally:\n",
        "              #   for h in hooks:\n",
        "              #     h.remove()\n",
        "\n",
        "\n",
        "            # Stop if the end-of-sequence token is generated\n",
        "            if next_token_id.item() == tokenizer.eos_token_id:\n",
        "                if patch_activations:\n",
        "                  print('clearing hook')\n",
        "                  for h in hooks:\n",
        "                    h.remove()\n",
        "                break\n",
        "\n",
        "    # Prepare the output dictionary\n",
        "    output_dict = {\n",
        "        \"sequences\": sequences,\n",
        "        # \"logits\": last_logits,\n",
        "        # \"attentions\": last_attentions,\n",
        "        # \"hidden_states\": last_hidden_states\n",
        "    }\n",
        "\n",
        "    # Clear GPU memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return output_dict"
      ],
      "metadata": {
        "id": "xJgMH5ZWyjY4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "purtubed_prompt"
      ],
      "metadata": {
        "id": "vgu8-7vQuQvP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activations = inference_with_hidden_state_new(purtubed_prompt, save_activations=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG4wI_6nsyPE",
        "outputId": "db057e56-9aa2-4c00-d865-e7fb7c0beee8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving activations ..\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activations.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmyRSXxXtH8n",
        "outputId": "87b772f2-8aba-4081-a21e-d4cea4b2abf2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['layer_0', 'layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_20', 'layer_21', 'layer_22', 'layer_23', 'layer_24', 'layer_25', 'layer_26', 'layer_27'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans = inference_with_hidden_state_new(correct_prompt)"
      ],
      "metadata": {
        "id": "Ox1_a72Au9bE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ans['sequences'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "CC7dcYSIu9Ye",
        "outputId": "580b507b-91f2-4cd4-c6d3-01b38ec4865f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<|im_end|>\\n<|im_start|>user\\nChenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?<|im_end|>\\n<|im_start|>assistant\\nTo determine Anne's age, we need to follow these steps:\\n\\n1. Find out Alyana's age.\\n2. Use Alyana's age to find out Anne's age.\\n\\nStep 1: Calculate Alyana's age.\\nChenny is 64 years old, and Alyana is 54 years younger than Chenny. Therefore, Alyana's age is:\\n\\\\[ 64 - 54 = 10 \\\\]\\nSo, Alyana is 10 years old.\\n\\nStep 2: Calculate Anne's age.\\nAnne is 55 years older than Alyana. Therefore, Anne's age is:\\n\\\\[ 10 + 55 = 65 \\\\]\\nSo, Anne is 65 years old.\\n\\nThe final answer is:\\n\\\\[\\n\\\\boxed{65}\\n\\\\]<|im_end|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans1 = inference_with_hidden_state_new(correct_prompt,patch_activations=True, patch_layer=0, activations = activations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cLIhmryu9Vm",
        "outputId": "377a7316-6c0e-4570-aae3-2c40287d270e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "applying patching\n",
            "clearing hook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ans1['sequences'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "73kYAX2du9TV",
        "outputId": "b730a16b-f9e1-44a9-9412-c4d567db1d3d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<|im_end|>\\n<|im_start|>user\\nChenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?<|im_end|>\\n<|im_start|>assistant\\nTo determine Anne's age, we need to follow these steps:\\n\\n1. Find Alyana's age.\\n2. Use Alyana's age to find Anne's age.\\n\\nFirst, let's find Alyana's age. We know that Alyana is 54 years younger than Chenny. Chenny is 64 years old. Therefore, Alyana's age is:\\n\\\\[ 64 - 54 = 10 \\\\]\\nSo, Alyana is 10 years old.\\n\\nNext, we need to find Anne's age. We know that Anne is 55 years older than Alyana. Since Alyana is 10 years old, Anne's age is:\\n\\\\[ 10 + 55 = 65 \\\\]\\nSo, Anne is 65 years old.\\n\\nTherefore, the answer is:\\n\\\\[\\n\\\\boxed{65}\\n\\\\```<|im_end|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indirect_effect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYS-rgTk2DZB",
        "outputId": "bcecd5f1-7971-45f5-e301-e2688a0ff369"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pr_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr!_0': tensor(2.3648e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr!_1': tensor(3.7947e-17, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_0_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_0!_0': tensor(5.0182e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_0_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_0!_1': tensor(8.0838e-16, device='cuda:0', dtype=torch.bfloat16)}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCxFuy5v4N0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the probability for correct and purturbed answers from the clean run\n",
        "output_clean = inference_with_hidden_state_new(corret_prompt)"
      ],
      "metadata": {
        "id": "gJO95Z84s9Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indirect_effect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EHEqZLgs9NX",
        "outputId": "228b9fa0-7e81-4d00-8dcf-fda032715435"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|def run_indirect_effect():\n",
        "\n",
        "  # get the activations from a purtubed prompt\n",
        "  activations = inference_with_hidden_state_new(purtubed_prompt, save_activations=True)\n",
        "\n",
        "  # get the probability for correct and purturbed answers from the clean run\n",
        "  output_clean = inference_with_hidden_state_new(corret_prompt)\n",
        "\n",
        "  # get the probability for correct and purturbed answers from the patch run for each layer in the model\n",
        "  for layer in range(model.config.num_hidden_layers):\n",
        "    inference_with_hidden_state_new(correct_prompt,patch_activations=True, patch_layer=layer, stored_activations = activations)\n",
        ""
      ],
      "metadata": {
        "id": "TNzHZIaY7Yio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "Mvb2hEhY72Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDdncOLa8GjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb5AgTUa8iFL",
        "outputId": "22f4de80-5483-430b-a8c9-d13103f00ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 1536)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "isinstance(model.model.layers[1].mlp, nn.Module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqTQeDch8qoB",
        "outputId": "b3ad26f7-3bcf-497a-f432-4f43facf6fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stored_activations = []"
      ],
      "metadata": {
        "id": "ziZ67IEh_IAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qDbW5NJG-aG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5bUtz5PO9KP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lets_see = inference_with_hidden_state_new(purtubed_prompt, save_activations=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGhGMj-yoJre",
        "outputId": "1e7ffae2-137e-4ed3-f454-881828174dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "applying\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lets_see['layer_0'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGnxlbsmEGZu",
        "outputId": "f8a1e724-5d84-4034-8821-7328de9c5251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 245, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqrdOv3IEGWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WZbTMLBEGU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exec_circuit_find():\n",
        "\n",
        "  activations = inference_with_\n",
        "\n"
      ],
      "metadata": {
        "id": "qFb7sEp5sosl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXAp1AVksop0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q8qxajfhsohP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([90])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yvd79fQWqNI8",
        "outputId": "d2580b56-e40b-423e-f32d-f2d19dd3ed71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjWDhbfrqEio",
        "outputId": "69774592-ae0e-4f49-9a2e-128b37349805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.6213e-22, 2.4945e-24, 5.4528e-21,  ..., 3.8774e-24, 3.8774e-24,\n",
              "         3.8774e-24]], device='cuda:0', dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input = \"Chenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?\"\n",
        "ans = inference_with_hidden_state_new(correct_prompt, top_p =1.0)\n"
      ],
      "metadata": {
        "id": "OwVElvEVbsD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTMraLCmpC30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indirect_effect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFzLo4eJdRXC",
        "outputId": "d5dba517-fbaf-4cdf-c4f3-7b6b3ca8bbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pr_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr!_0': tensor(1.1990e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr!_1': tensor(3.8164e-16, device='cuda:0', dtype=torch.bfloat16)}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boxed_token = torch.tensor((tokenizer.encode('boxed{')))"
      ],
      "metadata": {
        "id": "z8uprY8aeYLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2,len(ans['sequences'].squeeze())):\n",
        "  sample = ans['sequences'][:, :i+1]\n",
        "  # print(smple)\n",
        "  # if sample[:, -2:]== boxed_token.to(device):\n",
        "  #   print('yes', i)\n",
        "\n",
        "\n",
        "  if torch.all(sample[0,-2:] == boxed_token.to('cuda')):\n",
        "    print('yoo')\n",
        "    print(i)\n",
        "    start_tracking = True\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6yw7KF3dnCc",
        "outputId": "5dce464f-6733-46da-8e70-b24498a97897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yoo\n",
            "244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(ans['sequences'].squeeze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "xMhvjdj5dAb5",
        "outputId": "3c3023d3-3c85-4bef-abfc-504536a0e639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'builtin_function_or_method' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-00dfee5b047f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'builtin_function_or_method' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ans['sequences'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "kPtss4iEbw3R",
        "outputId": "52b3da25-dbe0-4592-bff2-c7884b198148"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ans' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f238e8640989>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ans' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ans['sequences'].squeeze()[244])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YBwrCGfXcP7w",
        "outputId": "7e09e0d2-475d-4a69-be74-453abf0c48ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class ActivationPatcher:\n",
        "    def __init__(self, model, tokenizer, device='cuda'):\n",
        "        \"\"\"Initialize the ActivationPatcher with model and tokenizer.\n",
        "\n",
        "        Args:\n",
        "            model: The language model to use\n",
        "            tokenizer: The tokenizer for the model\n",
        "            device: Device to run on (default: 'cuda')\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.sample_probs = None\n",
        "        self.indirect_effect = {}\n",
        "        self.boxed_token = None  # This needs to be set\n",
        "\n",
        "    def prepare_input(self, prompt):\n",
        "        \"\"\"Convert a text prompt to model inputs.\n",
        "\n",
        "        Args:\n",
        "            prompt: Text prompt to convert\n",
        "\n",
        "        Returns:\n",
        "            Tokenized input IDs\n",
        "        \"\"\"\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
        "        return model_inputs.input_ids\n",
        "\n",
        "    def inference(\n",
        "        self,\n",
        "        prompt,\n",
        "        temperature=0.7,\n",
        "        top_p=0.8,\n",
        "        save_activations=False,\n",
        "        patch_activations=False,\n",
        "        patch_layer=0,\n",
        "        activations=None,\n",
        "        max_tokens=512\n",
        "    ):\n",
        "        \"\"\"Run inference with options for saving or patching MLP activations.\n",
        "\n",
        "        Args:\n",
        "            prompt: Text prompt to process\n",
        "            temperature: Sampling temperature\n",
        "            top_p: Top-p sampling parameter\n",
        "            save_activations: Whether to save activations\n",
        "            patch_activations: Whether to patch activations\n",
        "            patch_layer: Which layer to patch if patching\n",
        "            activations: Pre-saved activations to use for patching\n",
        "            max_tokens: Maximum tokens to generate\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with outputs including generated sequences\n",
        "        \"\"\"\n",
        "        # Initialize\n",
        "        input_ids = self.prepare_input(prompt)\n",
        "        sequences = input_ids\n",
        "        stored_activations = {}\n",
        "\n",
        "        # For tracking boxed answers\n",
        "        track = False\n",
        "        cnt = 0\n",
        "        hooks = []\n",
        "\n",
        "        # Configure model\n",
        "        self.model.config.use_cache = True\n",
        "\n",
        "        # Generate tokens\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_tokens):\n",
        "                outputs = self.model(\n",
        "                    input_ids=sequences,\n",
        "                )\n",
        "\n",
        "                # Get logits and sample next token\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "                next_token_logits = next_token_logits / temperature\n",
        "                probs = torch.softmax(next_token_logits, dim=-1)\n",
        "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # Add new token to sequence\n",
        "                sequences = torch.cat([sequences, next_token_id], dim=-1)\n",
        "\n",
        "                # Track probabilities for correct and perturbed answers\n",
        "                if track and cnt < len(self.correct_ans_tokens):\n",
        "                    pr = f'pr*_{patch_layer}' if patch_activations else 'pr'\n",
        "                    self.indirect_effect[f'{pr}_{cnt}'] = probs[0, self.correct_ans_tokens[cnt]]\n",
        "                    self.indirect_effect[f'{pr}!_{cnt}'] = probs[0, self.perturbed_ans_tokens[cnt]]\n",
        "                    cnt += 1\n",
        "\n",
        "                # Detect when we enter a boxed answer\n",
        "                if torch.all(sequences[0, -2:] == self.boxed_token.to(self.device)):\n",
        "                    track = True\n",
        "                    cnt = 0\n",
        "                    # self.sample_probs = probs\n",
        "\n",
        "                # Detect end of boxed answer\n",
        "                if track and torch.all(sequences[0, -1:] == torch.tensor(self.tokenizer.encode('}')).to(self.device)):\n",
        "                    track = False\n",
        "                    print('End of boxed answer detected')\n",
        "\n",
        "                # Handle activation saving\n",
        "                if save_activations and track and cnt == 0:\n",
        "                    print('Saving activations...')\n",
        "                    stored_activations = self._save_layer_activations(sequences)\n",
        "                    return stored_activations\n",
        "\n",
        "                # Handle activation patching\n",
        "                if patch_activations and track and cnt == 0:\n",
        "                    print(f'Applying patching at layer {patch_layer}')\n",
        "                    hooks = self._apply_activation_patch(patch_layer, activations)\n",
        "\n",
        "                # Stop on EOS token\n",
        "                if next_token_id.item() == self.tokenizer.eos_token_id:\n",
        "                    if patch_activations and hooks:\n",
        "                        print('Clearing hooks')\n",
        "                        for h in hooks:\n",
        "                            h.remove()\n",
        "                    break\n",
        "\n",
        "        # Clean up\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return {\n",
        "            \"sequences\": sequences,\n",
        "            \"indirect_effect\": self.indirect_effect\n",
        "        }\n",
        "\n",
        "    def _save_layer_activations(self, sequences):\n",
        "        \"\"\"Save MLP activations for all layers.\n",
        "\n",
        "        Args:\n",
        "            sequences: Current token sequences\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of activations by layer\n",
        "        \"\"\"\n",
        "        stored_activations = {}\n",
        "        hooks = []\n",
        "\n",
        "        try:\n",
        "            # Create hooks for each layer\n",
        "            for layer in range(self.model.config.num_hidden_layers):\n",
        "                print(f\"Saving activations for layer {layer}\")\n",
        "\n",
        "                def hook_fn(name):\n",
        "                    def hook(module, input, output):\n",
        "                        stored_activations[name] = output.detach().clone()\n",
        "                    return hook\n",
        "\n",
        "                module = self.model.model.layers[layer].mlp\n",
        "                handle = module.register_forward_hook(hook_fn(f'layer_{layer}'))\n",
        "                hooks.append(handle)\n",
        "\n",
        "            # Run forward pass to collect activations\n",
        "            self.model(input_ids=sequences)\n",
        "\n",
        "        finally:\n",
        "            # Remove hooks when done\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "\n",
        "        return stored_activations\n",
        "\n",
        "    def _apply_activation_patch(self, patch_layer, activations):\n",
        "        \"\"\"Apply activation patching to a specific layer.\n",
        "\n",
        "        Args:\n",
        "            patch_layer: Layer to patch\n",
        "            activations: Saved activations to use for patching\n",
        "\n",
        "        Returns:\n",
        "            List of hook handles\n",
        "        \"\"\"\n",
        "        hooks = []\n",
        "\n",
        "        def hook_fn(name):\n",
        "            def hook(module, input, output):\n",
        "                output[:, -1, :] = activations[f'layer_{patch_layer}'][:, -1, :]\n",
        "                return output\n",
        "            return hook\n",
        "\n",
        "        module = self.model.model.layers[patch_layer].mlp\n",
        "        handle = module.register_forward_hook(hook_fn(f'layer_{patch_layer}'))\n",
        "        hooks.append(handle)\n",
        "\n",
        "        return hooks\n",
        "\n",
        "    def run_indirect_effect_analysis(\n",
        "        self,\n",
        "        correct_prompt,\n",
        "        perturbed_prompt,\n",
        "        correct_ans_tokens,\n",
        "        perturbed_ans_tokens\n",
        "    ):\n",
        "        \"\"\"Run full indirect effect analysis with patching.\n",
        "\n",
        "        Args:\n",
        "            correct_prompt: The original correct prompt\n",
        "            perturbed_prompt: The modified prompt with perturbations\n",
        "            correct_ans_tokens: Token IDs for correct answers\n",
        "            perturbed_ans_tokens: Token IDs for perturbed answers\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with analysis results\n",
        "        \"\"\"\n",
        "        # Set up answer tokens\n",
        "        self.correct_ans_tokens = correct_ans_tokens\n",
        "        self.perturbed_ans_tokens = perturbed_ans_tokens\n",
        "\n",
        "        # Step 1: Get activations from perturbed prompt\n",
        "        print(\"Getting activations from perturbed prompt...\")\n",
        "        activations = self.inference(perturbed_prompt, save_activations=True)\n",
        "\n",
        "        # Step 2: Run clean inference on correct prompt\n",
        "        print(\"Running inference on clean prompt...\")\n",
        "        clean_results = self.inference(correct_prompt)\n",
        "\n",
        "        # Step 3: Run patched inference for each layer\n",
        "        all_results = {\"clean\": clean_results}\n",
        "        for layer in range(self.model.config.num_hidden_layers):\n",
        "            print(f\"Testing indirect effect at layer {layer}...\")\n",
        "            patched_results = self.inference(\n",
        "                correct_prompt,\n",
        "                patch_activations=True,\n",
        "                patch_layer=layer,\n",
        "                activations=activations\n",
        "            )\n",
        "            all_results[f\"patched_layer_{layer}\"] = patched_results\n",
        "\n",
        "        return all_results\n",
        "\n"
      ],
      "metadata": {
        "id": "7PqCc4lBb9b-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_prompt = \"Chenny is 64 years old. Alyana is 54 years younger than Chenny. How old is Anne if she is 55 years older than Alyana?\"\n",
        "perturbed_prompt = \"Chenny is 78 years old. Alyana is 21 years younger than Chenny. How old is Anne if she is 31 years older than Alyana?\"\n",
        "\n",
        "correct_ans = '65'\n",
        "perturbed_ans = '88'\n",
        "\n",
        "correct_ans_tokens = tokenizer.encode(str(correct_ans))\n",
        "purturbed_ans_tokens = tokenizer.encode(str(perturbed_ans))\n"
      ],
      "metadata": {
        "id": "d3zta5sd8r59"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patcher = ActivationPatcher(model, tokenizer)\n",
        "\n",
        "# Set up the boxed token if needed\n",
        "patcher.boxed_token = torch.tensor(tokenizer.encode('boxed{')).to('cuda')"
      ],
      "metadata": {
        "id": "00yxVDXbcN_r"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = patcher.run_indirect_effect_analysis(\n",
        "    correct_prompt=correct_prompt,\n",
        "    perturbed_prompt=perturbed_prompt,\n",
        "    correct_ans_tokens=correct_ans_tokens,\n",
        "    perturbed_ans_tokens=purturbed_ans_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "qXH_HG_FccV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b223f7-a3f0-4314-f04a-23e11f6219a2"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting activations from perturbed prompt...\n",
            "Saving activations...\n",
            "Saving activations for layer 0\n",
            "Saving activations for layer 1\n",
            "Saving activations for layer 2\n",
            "Saving activations for layer 3\n",
            "Saving activations for layer 4\n",
            "Saving activations for layer 5\n",
            "Saving activations for layer 6\n",
            "Saving activations for layer 7\n",
            "Saving activations for layer 8\n",
            "Saving activations for layer 9\n",
            "Saving activations for layer 10\n",
            "Saving activations for layer 11\n",
            "Saving activations for layer 12\n",
            "Saving activations for layer 13\n",
            "Saving activations for layer 14\n",
            "Saving activations for layer 15\n",
            "Saving activations for layer 16\n",
            "Saving activations for layer 17\n",
            "Saving activations for layer 18\n",
            "Saving activations for layer 19\n",
            "Saving activations for layer 20\n",
            "Saving activations for layer 21\n",
            "Saving activations for layer 22\n",
            "Saving activations for layer 23\n",
            "Saving activations for layer 24\n",
            "Saving activations for layer 25\n",
            "Saving activations for layer 26\n",
            "Saving activations for layer 27\n",
            "Running inference on clean prompt...\n",
            "End of boxed answer detected\n",
            "Testing indirect effect at layer 0...\n",
            "Applying patching at layer 0\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 1...\n",
            "Applying patching at layer 1\n",
            "End of boxed answer detected\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 2...\n",
            "Applying patching at layer 2\n",
            "End of boxed answer detected\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 3...\n",
            "Applying patching at layer 3\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 4...\n",
            "Applying patching at layer 4\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 5...\n",
            "Applying patching at layer 5\n",
            "End of boxed answer detected\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 6...\n",
            "Applying patching at layer 6\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 7...\n",
            "Applying patching at layer 7\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 8...\n",
            "Applying patching at layer 8\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 9...\n",
            "Applying patching at layer 9\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 10...\n",
            "Applying patching at layer 10\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 11...\n",
            "Applying patching at layer 11\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 12...\n",
            "Applying patching at layer 12\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 13...\n",
            "Applying patching at layer 13\n",
            "End of boxed answer detected\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 14...\n",
            "Applying patching at layer 14\n",
            "End of boxed answer detected\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 15...\n",
            "Applying patching at layer 15\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 16...\n",
            "Applying patching at layer 16\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 17...\n",
            "Applying patching at layer 17\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 18...\n",
            "Applying patching at layer 18\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 19...\n",
            "Applying patching at layer 19\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 20...\n",
            "Applying patching at layer 20\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 21...\n",
            "Applying patching at layer 21\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 22...\n",
            "Applying patching at layer 22\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 23...\n",
            "Applying patching at layer 23\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 24...\n",
            "Applying patching at layer 24\n",
            "End of boxed answer detected\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 25...\n",
            "Applying patching at layer 25\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 26...\n",
            "Applying patching at layer 26\n",
            "Clearing hooks\n",
            "Testing indirect effect at layer 27...\n",
            "Applying patching at layer 27\n",
            "Clearing hooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patcher.indirect_effect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMadEYJz8HGW",
        "outputId": "d7a76b85-678a-4e98-e13e-6ed94c2ac27c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pr_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr!_0': tensor(7.8515e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr!_1': tensor(8.1601e-15, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_0_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_0!_0': tensor(9.3703e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_0_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_0!_1': tensor(1.4322e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_1_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_1!_0': tensor(4.8431e-11, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_1_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_1!_1': tensor(1.0800e-11, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_2_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_2!_0': tensor(1.0090e-12, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_2_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_2!_1': tensor(8.1601e-15, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_3_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_3!_0': tensor(6.4393e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_3_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_3!_1': tensor(5.9328e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_4_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_4!_0': tensor(2.2471e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_4_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_4!_1': tensor(3.3827e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_5_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_5!_0': tensor(1.0090e-12, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_5_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_5!_1': tensor(5.6066e-15, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_6_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_6!_0': tensor(1.9806e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_6_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_6!_1': tensor(4.6144e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_7_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_7!_0': tensor(6.8390e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_7_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_7!_1': tensor(3.8164e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_8_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_8!_0': tensor(5.6843e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_8_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_8!_1': tensor(6.6353e-17, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_9_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_9!_0': tensor(3.6948e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_9_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_9!_1': tensor(2.9837e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_10_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_10!_0': tensor(1.5454e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_10_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_10!_1': tensor(4.6144e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_11_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_11!_0': tensor(8.8818e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_11_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_11!_1': tensor(1.7497e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_12_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_12!_0': tensor(7.7716e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_12_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_12!_1': tensor(1.1796e-15, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_13_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_13!_0': tensor(1.0090e-12, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_13_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_13!_1': tensor(9.8810e-15, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_14_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_14!_0': tensor(1.6556e-12, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_14_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_14!_1': tensor(2.6368e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_15_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_15!_0': tensor(6.0396e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_15_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_15!_1': tensor(6.2450e-17, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_16_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_16!_0': tensor(1.1879e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_16_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_16!_1': tensor(4.8572e-17, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_17_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_17!_0': tensor(6.0396e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_17_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_17!_1': tensor(1.5005e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_18_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_18!_0': tensor(5.0182e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_18_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_18!_1': tensor(6.6960e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_19_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_19!_0': tensor(1.6209e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_19_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_19!_1': tensor(8.0231e-17, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_20_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_20!_0': tensor(8.2601e-14, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_20_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_20!_1': tensor(1.0972e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_21_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_21!_0': tensor(1.8652e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_21_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_21!_1': tensor(7.1471e-16, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_22_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_22!_0': tensor(4.7606e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_22_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_22!_1': tensor(4.1078e-15, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_23_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_23!_0': tensor(1.8758e-12, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_23_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_23!_1': tensor(9.8810e-15, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_24_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_24!_0': tensor(2.9331e-11, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_24_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_24!_1': tensor(9.5497e-12, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_25_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_25!_0': tensor(3.6554e-08, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_25_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_25!_1': tensor(8.5856e-10, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_26_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_26!_0': tensor(2.1316e-12, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_26_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_26!_1': tensor(6.1107e-13, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_27_0': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_27!_0': tensor(3.5834e-10, device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_27_1': tensor(1., device='cuda:0', dtype=torch.bfloat16),\n",
              " 'pr*_27!_1': tensor(4.8431e-11, device='cuda:0', dtype=torch.bfloat16)}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8R3Su2QTEin_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}